version: '3.8'

services:
  # The DeepStream Service
  deepstream:
    image: nvcr.io/nvidia/deepstream:6.4-triton
    container_name: deepstream_container
    runtime: nvidia
    working_dir: /app
    volumes:
      # Mount your local deepstream_app directory to /app inside the container
      - ./deepstream_app:/app
      # Mount the shared volume for metadata output
      - metadata_volume:/app/metadata
    # Command to run your Python script when the container starts
    command: python3 deepstream-camera.py -c config.txt
  # The Ollama and Streamlit Service
  ollama_app:
    build: ./ollama_app # Path to your Dockerfile for this service
    container_name: ollama_container
    ports:
      - "8501:8501"   # Streamlit port
      - "11434:11434" # Ollama port
    volumes:
      # Mount the same shared volume to a path inside this container
      - metadata_volume:/app/metadata
      - ollama_data:/root/.ollama # Persist Ollama models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

# Define the shared volume
volumes:
  metadata_volume:
  ollama_data:
